## Question 1

(1)$首先计算经验熵H(D) = -\frac{9}{15}\log_2\frac{9}{15} - \frac{6}{15}\log_2\frac{6}{15} = 0.971\\$
然后计算各特征对数据集D的信息增益，分别以$A_1,A_2,A_3,A_4$表示年龄，有工作、有自己的房子和信贷情况4个特征，则计算四个特征的信息增益过程如下：($D_1,D_2,D_3$分别是D中特征取值为青年、中年和老年的样本子集)  
$$
g(D,A_1) = H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)]=0.083\\
g(D,A_2) = H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)]=0.324\\
g(D,A_3) = H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)]
=0.420\\
g(D,A_4) = H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)]
=0.363$$
(2)用ID3算法建立决策树,就是用信息增益来选择每次分类的特征。由于A3的信息增益最大，所以设置特征A3作为根节点的特征，它将训练数据集D划分为两个子集$D_1(A_3取值为“是”)和D_2(A_3取值为“否”),由于A_3取“是”时结果为确定的，所以D_1时叶子节点$，对于$D_2则要从A_1,A_2,A_4中选取新的特征，计算各个特征的信息增益$
$$
g(D_2,A_1) = H(D_2)-H(D_2|A_1) = 0.918-0.667=0.251\\
g(D_2,A_2) = H(D_2)-H(D_2|A_2) = 0.918\\
g(D_2,A_4) = H(D_2)-H(D_2|A_4) = 0.474\\
$$
选择信息增益最大的特征$A_2(有工作)$作为节点的特征，由于$A_2$有两个可能取值，从这一结点引出两个子结点，一个对应“有工作”的子结点，包含3个样本，它们属于同一类，所以这是一个叶结点，类标记为“是”；另一个是对应“无工作”的子结点，包含6个样本，他们也属于同一类，这也是一个叶节点，类标记为“否”。
![avatar](决策树.PNG) 
## Question 2
 $\textbf{用伪代码描述一种决策树剪枝的方法.}$  
 答：树的剪枝算法伪代码如下：  
输入：生成算法产生的整个树T，参数$\alpha$；  
输出：修剪后的子树$T_\alpha$。  
(1)计算每个节点的经验熵。  
(2)递归地从树的叶结点向上回溯。设一组叶结点回溯到其父结点之前与之后的整体树分别为$T_B和T_A$，其对应的损失函数值分别是$C_\alpha(T_B)和C_\alpha(T_A)$，如果$C_\alpha(T_A)\leq C_\alpha(T_B)$则进行剪枝，即将父结点变为新的叶结点。    
(3)返回(2)，直至不能继续为止，得到损失函数最小的子树$T_\alpha$。  
其中损失函数可以定义为$C_\alpha(T)=\varSigma_{t=1}^{|T|}N_tH_t(T)+\alpha |T|$
$H_t(T)为经验熵，|T|代表决策树的叶结点个数，即代表了树的复杂度。$

## Question 3
有N个样本x_1,…, x_N，每个样本维数D，希望将样本维数降低到K，请给出PCA算法的计算过程.  
答：对N个样本计算协方差矩阵$\varSigma$，矩阵大小为$D\times D$，求出$\varSigma$的所有特征值并从大到小排序，求出前K个特征值，并求出这K个特征值对应的特征向量，构造矩阵T大小为$D\times K$，则N个样本的主成分为$Y=T'X$，Y即为样本降维后的主成分，至此完成PCA算法的整个过程。